{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47efdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from processing import utils as p_utils\n",
    "import contextlib\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "root = Path.cwd().parent\n",
    "library = root.parent / \"reward-surfaces\"\n",
    "stash = root / \"stash\"\n",
    "scripts = library / \"scripts\"\n",
    "\n",
    "run_suffix = \"_test\"\n",
    "env_name = \"CartPole-v1\"\n",
    "agent_name = \"SB3_ON\"\n",
    "run_id = \"cartpole\" + run_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cfb338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "False\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 8 environments\n",
      "cpu\n",
      "Log path: /home/mattis/git/sem5/reward_landscapes/stash/cartpole_test/agent/ppo/CartPole-v1_5\n",
      "saved checkpoint 0010000\n",
      "Eval num_timesteps=10000, episode_reward=126.30 +/- 28.32\n",
      "Episode length: 126.30 +/- 28.32\n",
      "New best mean reward!\n",
      "saved checkpoint 0020000\n",
      "Eval num_timesteps=20000, episode_reward=269.36 +/- 48.96\n",
      "Episode length: 269.36 +/- 48.96\n",
      "New best mean reward!\n",
      "saved checkpoint 0030000\n",
      "Eval num_timesteps=30000, episode_reward=470.74 +/- 76.16\n",
      "Episode length: 470.74 +/- 76.16\n",
      "New best mean reward!\n",
      "saved checkpoint 0040000\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "saved checkpoint 0050000\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "# Train RL agent\n",
    "script_name = \"train_agent\"\n",
    "args = {\n",
    "    \"save_dir\": stash / run_id / \"agent\",\n",
    "    \"agent_name\": agent_name,\n",
    "    \"env\": env_name,\n",
    "    \"device\": device,\n",
    "    \"hyperparameters\": '{\"ALGO\": \"PPO\", \"n_timesteps\":50000}',\n",
    "}\n",
    "kwargs = {\n",
    "    \"--save_freq\": str(10000),\n",
    "}\n",
    "p_utils.execute(name=script_name, args=args, kwargs=kwargs, cwd=library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727cd3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "['0010000', '0020000', '0030000', '0040000', '0050000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 8 environments\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 8 environments\n",
      "cpu\n",
      "cpu\n",
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 8 environments\n",
      "cpu\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 8 environments\n",
      "cpu\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 8 environments\n",
      "cpu\n",
      "Log path: /home/mattis/git/sem5/reward_landscapes/stash/cartpole_test/gradient/ppo/CartPole-v1_1\n",
      "evaluating  0030000\n",
      "computing rollout with 1000000 steps, 10000000000000 episodes\n",
      "Gathering data\n",
      "Log path: /home/mattis/git/sem5/reward_landscapes/stash/cartpole_test/gradient/ppo/CartPole-v1_1\n",
      "evaluating  0010000\n",
      "computing rollout with 1000000 steps, 10000000000000 episodes\n",
      "Gathering data\n",
      "Log path: /home/mattis/git/sem5/reward_landscapes/stash/cartpole_test/gradient/ppo/CartPole-v1_2\n",
      "evaluating  0040000\n",
      "computing rollout with 1000000 steps, 10000000000000 episodes\n",
      "Gathering data\n",
      "Log path: /home/mattis/git/sem5/reward_landscapes/stash/cartpole_test/gradient/ppo/CartPole-v1_2\n",
      "evaluating  0050000\n",
      "computing rollout with 1000000 steps, 10000000000000 episodes\n",
      "Gathering data\n",
      "Log path: /home/mattis/git/sem5/reward_landscapes/stash/cartpole_test/gradient/ppo/CartPole-v1_3\n",
      "evaluating  0020000\n",
      "computing rollout with 1000000 steps, 10000000000000 episodes\n",
      "Gathering data\n",
      "computing batch grad mag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py:281: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  torch.squeeze(torch.tensor(test_states[0][0:2], device=device), dim=1),\n",
      "/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py:281: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  torch.squeeze(torch.tensor(test_states[0][0:2], device=device), dim=1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing batch grad mag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py:281: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  torch.squeeze(torch.tensor(test_states[0][0:2], device=device), dim=1),\n",
      "/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py:281: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  torch.squeeze(torch.tensor(test_states[0][0:2], device=device), dim=1),\n",
      "/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py:281: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  torch.squeeze(torch.tensor(test_states[0][0:2], device=device), dim=1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing batch grad mag\n",
      "computing batch grad mag\n",
      "computing batch grad mag\n",
      "Aborted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/scripts/run_jobs_multiproc.py\", line 16, in <module>\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 54, in <module>\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 54, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    Traceback (most recent call last):\n",
      "return _run_code(code, main_globals, None,\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 54, in <module>\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/mattis/.pyenv/versions/3.8.20/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 54, in <module>\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 54, in <module>\n",
      "    main()\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/scripts/run_jobs_multiproc.py\", line 12, in main\n",
      "    run_job_list(args.jobs_fname, args.num_cpus)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/runners/run_jobs_multiproc.py\", line 37, in run_job_list\n",
      "    main()\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 50, in main\n",
      "    main()\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 50, in main\n",
      "    save_results(agent, info, out_dir, cur_results, job_name)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/utils/compute_results.py\", line 36, in save_results\n",
      "    save_results(agent, info, out_dir, cur_results, job_name)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/utils/compute_results.py\", line 36, in save_results\n",
      "    main()\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 50, in main\n",
      "    main()\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 50, in main\n",
      "    save_results(agent, info, out_dir, cur_results, job_name)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/utils/compute_results.py\", line 36, in save_results\n",
      "    save_results(agent, info, out_dir, cur_results, job_name)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/utils/compute_results.py\", line 36, in save_results\n",
      "    main()\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/bin/eval_tradj.py\", line 50, in main\n",
      "    save_results(agent, info, out_dir, cur_results, job_name)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/utils/compute_results.py\", line 36, in save_results\n",
      "    run_job_list_list(job_list, num_cpus=num_cpus, disable_warnings=disable_warnings)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/runners/run_jobs_multiproc.py\", line 32, in run_job_list_list\n",
      "    policy_grad, _ = compute_policy_gradient_batch(evaluator, action_evaluator,\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 284, in compute_policy_gradient_batch\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "    policy_grad, _ = compute_policy_gradient_batch(evaluator, action_evaluator,\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 284, in compute_policy_gradient_batch\n",
      "    policy_grad, _ = compute_policy_gradient_batch(evaluator, action_evaluator,\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 284, in compute_policy_gradient_batch\n",
      "    policy_grad, _ = compute_policy_gradient_batch(evaluator, action_evaluator,\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 284, in compute_policy_gradient_batch\n",
      "    policy_grad, _ = compute_policy_gradient_batch(evaluator, action_evaluator,\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 284, in compute_policy_gradient_batch\n",
      "    grad_mag, grad_dir = compute_grad_mags_batch(evaluator, params, action_evaluator, num_episodes, num_steps)\n",
      "      File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 216, in compute_grad_mags_batch\n",
      "grad_mag, grad_dir = compute_grad_mags_batch(evaluator, params, action_evaluator, num_episodes, num_steps)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 234, in compute_grad_mags_batch\n",
      "    _, original_reward, done, value, state, act, info = evaluator._next_state_act() #, deterministic=True)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/agents/SB3/sb3_on_policy_train.py\", line 354, in _next_state_act\n",
      "    batch_states = torch.tensor(episode_states[idx:idx + clipped_batch_size], device=device)\n",
      "KeyboardInterrupt\n",
      "    grad_mag, grad_dir = compute_grad_mags_batch(evaluator, params, action_evaluator, num_episodes, num_steps)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 216, in compute_grad_mags_batch\n",
      "    _, original_reward, done, value, state, act, info = evaluator._next_state_act() #, deterministic=True)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/agents/SB3/sb3_on_policy_train.py\", line 354, in _next_state_act\n",
      "    grad_mag, grad_dir = compute_grad_mags_batch(evaluator, params, action_evaluator, num_episodes, num_steps)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 216, in compute_grad_mags_batch\n",
      "    grad_mag, grad_dir = compute_grad_mags_batch(evaluator, params, action_evaluator, num_episodes, num_steps)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/algorithms/eval_policy_hess.py\", line 216, in compute_grad_mags_batch\n",
      "    _, original_reward, done, value, state, act, info = evaluator._next_state_act() #, deterministic=True)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/agents/SB3/sb3_on_policy_train.py\", line 371, in _next_state_act\n",
      "    _, original_reward, done, value, state, act, info = evaluator._next_state_act() #, deterministic=True)\n",
      "  File \"/home/mattis/git/sem5/reward-surfaces/reward_surfaces/agents/SB3/sb3_on_policy_train.py\", line 354, in _next_state_act\n",
      "    self.state, rew, done, info = self.env.step(action)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\", line 206, in step\n",
      "    action, policy_val, policy_log_prob = policy_policy.forward(\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/policies.py\", line 655, in forward\n",
      "    action, policy_val, policy_log_prob = policy_policy.forward(\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/policies.py\", line 656, in forward\n",
      "    action, policy_val, policy_log_prob = policy_policy.forward(\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/policies.py\", line 656, in forward\n",
      "        log_prob = distribution.log_prob(actions)    log_prob = distribution.log_prob(actions)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/distributions.py\", line 292, in log_prob\n",
      "actions = distribution.get_actions(deterministic=deterministic)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/distributions.py\", line 89, in get_actions\n",
      "\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/distributions.py\", line 292, in log_prob\n",
      "    return self.distribution.log_prob(actions)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/categorical.py\", line 142, in log_prob\n",
      "    return self.distribution.log_prob(actions)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/categorical.py\", line 138, in log_prob\n",
      "    return self.sample()\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/distributions.py\", line 298, in sample\n",
      "    return self.distribution.sample()\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/categorical.py\", line 132, in sample\n",
      "    return self.step_wait()\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\", line 58, in step_wait\n",
      "    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/common/monitor.py\", line 94, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "    return log_pmf.gather(-1, value).squeeze(-1)\n",
      "KeyboardInterrupt\n",
      "    probs_2d = self.probs.reshape(-1, self._num_events)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/utils.py\", line 148, in __get__\n",
      "    self._validate_sample(value)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/distribution.py\", line 313, in _validate_sample\n",
      "    value = self.wrapped(instance)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/categorical.py\", line 101, in probs\n",
      "    return logits_to_probs(self.logits)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/utils.py\", line 90, in logits_to_probs\n",
      "    return F.softmax(logits, dim=-1)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/nn/functional.py\", line 1888, in softmax\n",
      "    valid = support.check(value)\n",
      "  File \"/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/torch/distributions/constraints.py\", line 276, in check\n",
      "    (value % 1 == 0) & (self.lower_bound <= value) & (value <= self.upper_bound)\n",
      "KeyboardInterrupt\n",
      "    ret = input.softmax(dim)\n",
      "KeyboardInterrupt\n",
      " 80%|████████  | 4/5 [00:46<00:11, 11.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define gradient eval jobs\n",
    "script_name = \"generate_eval_jobs\"\n",
    "args = {\n",
    "    \"train_dir\": stash / run_id / \"agent\",\n",
    "    \"out_dir\": stash / run_id / \"gradient\",\n",
    "}\n",
    "kwargs = {\n",
    "    \"--batch-grad\": None,\n",
    "    \"--num-steps\": str(1000000),\n",
    "    \"--device\": device,\n",
    "}\n",
    "shutil.rmtree(stash / run_id / \"gradient\")\n",
    "p_utils.execute(name=script_name, args=args, kwargs=kwargs, cwd=library)\n",
    "\n",
    "# Evaluate gradient direction\n",
    "script_name = \"run_jobs_multiproc\"\n",
    "args = {\n",
    "    \"job_dir\": stash / run_id / \"gradient\" / \"jobs.sh\",\n",
    "}\n",
    "kwargs = {\"--num-cpus\": str(7)}\n",
    "p_utils.execute(name=script_name, args=args, kwargs=kwargs, cwd=library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef6004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 8 environments\n",
      "cpu\n",
      "Log path: /home/mattis/git/sem5/reward_landscapes/stash/cartpole_test/surface/ppo/CartPole-v1_4\n",
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Define plane jobs\n",
    "script_name = \"generate_plane_jobs\"\n",
    "args = {\n",
    "    \"agent_dir\": stash / run_id / \"agent\" / \"best\",\n",
    "    \"out_dir\": stash / run_id / \"surface\",\n",
    "}\n",
    "kwargs = {\n",
    "    \"--grid-size\": \"31\",\n",
    "    \"--magnitude\": \"1.0\",\n",
    "    \"--num-steps\": str(200000),\n",
    "}\n",
    "# shutil.rmtree(stash / run_id / \"surface\")\n",
    "p_utils.execute(name=script_name, args=args, kwargs=kwargs, cwd=library)\n",
    "\n",
    "# Evaluate plane directions\n",
    "script_name = \"run_jobs_multiproc\"\n",
    "args = {\n",
    "    \"job_dir\": stash / run_id / \"surface\" / \"jobs.sh\",\n",
    "}\n",
    "kwargs = {\"--num-cpus\": str(7)}\n",
    "p_utils.execute(name=script_name, args=args, kwargs=kwargs, cwd=library)\n",
    "\n",
    "# Copy surface results to csv\n",
    "script_name = \"job_results_to_csv\"\n",
    "args = {\n",
    "    \"out_dir\": stash / run_id / \"surface\",\n",
    "}\n",
    "kwargs = {}\n",
    "p_utils.execute(name=script_name, args=args, kwargs=kwargs, cwd=library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mattis/.local/share/virtualenvs/reward_landscapes-JbenRfY6/lib/python3.8/site-packages/stable_baselines3/__init__.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['python3',\n",
       " 'scripts/plot_plane.py',\n",
       " './runs/cartpole_surface/results.csv--outname',\n",
       " './runs/cartpole--env_name',\n",
       " 'CartPole-v1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot reward surface\n",
    "script_name = \"plot_plane\"\n",
    "args = {\n",
    "    \"in_dir\": stash / run_id / \"surface\" / \"results.csv\",\n",
    "}\n",
    "kwargs = {\n",
    "    \"--outname\": stash / run_id / \"surface\",\n",
    "    \"--env_name\": \"CartPole-v1\",\n",
    "}\n",
    "p_utils.execute(name=script_name, args=args, kwargs=kwargs, cwd=library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94789b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate random directions\n",
    "[\n",
    "    \"python3\",\n",
    "    \"scripts/generate_plane_jobs.py\",\n",
    "    \"--grid-size=31\",\n",
    "    \"--magnitude=1.0\",\n",
    "    \"--num-steps\",\n",
    "    str(200000),\n",
    "    \"./runs/cartpole_checkpoints/best/\",\n",
    "    \"./runs/cartpole_surface/\",\n",
    "]\n",
    "[\n",
    "    \"python3\",\n",
    "    \"scripts/run_jobs_multiproc.py\",\n",
    "    \"--num-cpus\",\n",
    "    str(8),\n",
    "    \"./runs/cartpole_surface/jobs.sh\",\n",
    "]\n",
    "[\"python3\", \"scripts/job_results_to_csv.py\", \"./runs/cartpole_surface/\"]\n",
    "# Plot plane (random directions)\n",
    "[\n",
    "    \"python3\",\n",
    "    \"scripts/plot_plane.py\",\n",
    "    \"./runs/cartpole_surface/results.csv\" \"--outname\",\n",
    "    \"./runs/cartpole\" \"--env_name\",\n",
    "    \"CartPole-v1\",\n",
    "]\n",
    "[\n",
    "    \"python\",\n",
    "    \"scripts/eval_line_segment.py\",\n",
    "    \"./runs/cartpole_checkpoints/\",\n",
    "    \"./runs/eval_grad/cartpole/results\",\n",
    "    \"./runs/eval_line/cartpole/\" \"--num-episodes\",\n",
    "    str(100),\n",
    "    \"--length\",\n",
    "    str(20),\n",
    "    \"--max-magnitude\",\n",
    "    str(0.4),\n",
    "    \"--scale-dir\",\n",
    "]\n",
    "[\n",
    "    \"python\",\n",
    "    \"scripts/run_jobs_multiproc.py\",\n",
    "    \"--num-cpus=8\",\n",
    "    \"./runs/eval_line/cartpole/jobs.sh\",\n",
    "]\n",
    "[\"python\", \"scripts/job_results_to_csv.py\", \"./runs/eval_line/cartpole/\"]\n",
    "[\n",
    "    \"python\",\n",
    "    \"scripts/plot_eval_line_segement.py\",\n",
    "    \"./runs/eval_line/cartpole/results.csv\" \"--outname\",\n",
    "    \"cartpole__lines.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94fe98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reward_landscapes-JbenRfY6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
